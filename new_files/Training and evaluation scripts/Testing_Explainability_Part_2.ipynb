{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxULM0llEQjO",
        "outputId": "70bef9d9-782d-40b5-ea06-6242ad3b687b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Collecting transformers-interpret\n",
            "  Downloading transformers_interpret-0.10.0-py3-none-any.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Collecting captum>=0.3.1 (from transformers-interpret)\n",
            "  Downloading captum-0.8.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: ipython<8.0.0,>=7.31.1 in /usr/local/lib/python3.11/dist-packages (from transformers-interpret) (7.34.0)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from transformers-interpret) (4.51.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from captum>=0.3.1->transformers-interpret) (3.10.0)\n",
            "Collecting numpy<2.0 (from captum>=0.3.1->transformers-interpret)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from captum>=0.3.1->transformers-interpret) (24.2)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from captum>=0.3.1->transformers-interpret) (2.6.0+cu124)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython<8.0.0,>=7.31.1->transformers-interpret)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython<8.0.0,>=7.31.1->transformers-interpret) (4.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers-interpret) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers-interpret) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers-interpret) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers-interpret) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers-interpret) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers-interpret) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->transformers-interpret) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->transformers-interpret) (2025.3.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython<8.0.0,>=7.31.1->transformers-interpret) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython<8.0.0,>=7.31.1->transformers-interpret) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.31.1->transformers-interpret) (0.2.13)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers-interpret) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers-interpret) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers-interpret) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers-interpret) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers-interpret) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10->captum>=0.3.1->transformers-interpret)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers-interpret) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum>=0.3.1->transformers-interpret) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10->captum>=0.3.1->transformers-interpret) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers-interpret) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers-interpret) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers-interpret) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers-interpret) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers-interpret) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers-interpret) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum>=0.3.1->transformers-interpret) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.0.0->transformers-interpret) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=3.0.0->transformers-interpret) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->captum>=0.3.1->transformers-interpret) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->captum>=0.3.1->transformers-interpret) (3.0.2)\n",
            "Downloading transformers_interpret-0.10.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading captum-0.8.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, jedi, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, captum, transformers-interpret\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed captum-0.8.0 jedi-0.19.2 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 transformers-interpret-0.10.0\n"
          ]
        }
      ],
      "source": [
        "pip install openai transformers-interpret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9dIcEhAFfzd",
        "outputId": "ff1b1b35-9af2-40cb-9e75-b855e947ef47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7tNpOQJEY_v"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import torch\n",
        "import os\n",
        "from transformers_interpret import SequenceClassificationExplainer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"/content/drive/MyDrive/new_model/\"  # Replace with your model path or Hugging Face model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()\n",
        "\n",
        "cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n",
        "\n",
        "#Function to merge subword attributions\n",
        "def merge_subword_attributions(attributions):\n",
        "    merged_attributions = []\n",
        "    prev_word = None\n",
        "    accumulated_score = 0.0\n",
        "\n",
        "    for token, score in attributions:\n",
        "        clean_token = token.replace(\"##\", \"\")  # Remove subword markers\n",
        "\n",
        "        if token.startswith(\"##\") and prev_word:\n",
        "            prev_word += clean_token\n",
        "            accumulated_score += score\n",
        "        else:\n",
        "            if prev_word:\n",
        "                merged_attributions.append((prev_word, accumulated_score))\n",
        "            prev_word = clean_token\n",
        "            accumulated_score = score\n",
        "\n",
        "    if prev_word:\n",
        "        merged_attributions.append((prev_word, accumulated_score))\n",
        "\n",
        "    return merged_attributions\n",
        "\n",
        "#Function to apply a dynamic explanation threshold\n",
        "def should_explain(input_text, total_attribution_score, alpha=0.15):\n",
        "    num_words = len(input_text.split())\n",
        "    threshold = alpha * num_words  # Dynamic threshold based on input length\n",
        "    return total_attribution_score > threshold\n",
        "\n",
        "\n",
        "# Load API key securely\n",
        "\n",
        "OPENAI_API_KEY = \"...\"\n",
        "\n",
        "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "\n",
        "def generate_gpt_explanation(prediction, key_words, confidence):\n",
        "    prompt = f\"\"\"\n",
        "    The model classified this text as '{prediction}' with {confidence:.2%} confidence.\n",
        "    The most important words influencing this decision were: {', '.join(key_words)}.\n",
        "    Provide a simple explanation for a non-technical user.\n",
        "    \"\"\"\n",
        "\n",
        "    client = openai.OpenAI(api_key=openai.api_key)  #\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0  # Ensures consistent responses\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "#Main function to process text, explain, and generate output\n",
        "def explain_text(input_text, visualize=True):\n",
        "    #Get Model Prediction & Attributions\n",
        "    _ = cls_explainer(input_text)  # Generate explanations\n",
        "    attributions_list = cls_explainer.word_attributions\n",
        "    merged_attributions = merge_subword_attributions(attributions_list)\n",
        "\n",
        "    predicted_label = cls_explainer.predicted_class_name\n",
        "    confidence = cls_explainer.pred_probs.max().item()\n",
        "\n",
        "    total_attribution_score = sum(abs(score) for _, score in merged_attributions)\n",
        "    if not should_explain(input_text, total_attribution_score):\n",
        "        return f\"The model classified this as '{predicted_label}' with {confidence:.2%} confidence. No strong influencing words detected.\"\n",
        "\n",
        "    merged_attributions.sort(key=lambda x: abs(x[1]), reverse=True)\n",
        "    key_words = [word for word, score in merged_attributions[:5]]\n",
        "\n",
        "    explanation = generate_gpt_explanation(predicted_label, key_words, confidence)\n",
        "\n",
        "    if visualize:\n",
        "        cls_explainer.visualize()\n",
        "\n",
        "    return explanation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvcU-xaVNRt3"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "#replace this with your actual API key\n",
        "OPENAI_API_KEY = \"....\"\n",
        "\n",
        "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "def generate_gpt_explanation(prediction, key_words, confidence):\n",
        "    prompt = f\"\"\"\n",
        "    The model classified this text as '{prediction}' with {confidence:.2%} confidence.\n",
        "\n",
        "    The most important words influencing this decision were: {', '.join(key_words)}.\n",
        "\n",
        "    Provide a **concise, context-aware explanation** (50-80 words) about why these words suggest {prediction}.\n",
        "    - Explain **how these words are used in misinformation**.\n",
        "    - Provide **one real-world example** of misinformation using these words.\n",
        "    - Compare how these words are used in **real scientific news**.\n",
        "    - Make the explanation **brief and easy to understand**.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "        max_tokens=100\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sOH3NxyNdQ5",
        "outputId": "d05ddf46-f326-4a68-911a-839f953712a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In misinformation, the words \"cures\" and \"research\" are often used to make false claims about miracle treatments or breakthroughs that have not been scientifically proven. For example, a fake news article might claim that a certain herb cures cancer based on \"research\" that doesn't actually exist. In real scientific news, these words are used cautiously, with research findings undergoing rigorous testing and peer review before any claims of cures are made. Misinformation exploits these terms to spread false hope and misleading\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Example usage\n",
        "explanation = generate_gpt_explanation(\"Misinformation\", [\"cures\", \"research\"], 0.97)\n",
        "print(explanation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "wVBMWsxkmABd",
        "outputId": "2e890136-d7f9-4ab7-fc09-2e7d0f6942df"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_1 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_1</b></text></td><td><text style=\"padding-right:2em\"><b>-0.28</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 62%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> diabetes                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mis                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##form                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ation                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> can                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> be                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> harmful                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> spreading                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> false                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> claims                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> about                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> causes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> treatments                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cure                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##s                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟢 **This statement was classified as 'LABEL_1' with 99.86% confidence.**\n",
            "\n",
            "**The model identified the following words as important in making this decision:**\n",
            "- **\"diabetes\"** reinforced this classification.\n",
            "- **\"harmful\"** reinforced this classification.\n",
            "- **\",\"** reinforced this classification.\n",
            "\n",
            "**However, the following words slightly pushed against this classification:**\n",
            "- **\"misinformation\"** was present but did not strongly contribute.\n",
            "- **\"cures\"** was present but did not strongly contribute.\n",
            "\n",
            "📌 *This explanation is based on which words the model found important in making its prediction.*\n"
          ]
        }
      ],
      "source": [
        "def generate_explanation(input_text, visualize=True):\n",
        "\n",
        "    _ = cls_explainer(input_text)\n",
        "    attributions_list = cls_explainer.word_attributions\n",
        "\n",
        "    merged_attributions = merge_subword_attributions(attributions_list)\n",
        "\n",
        "    merged_attributions.sort(key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "    predicted_label = cls_explainer.predicted_class_name\n",
        "    confidence = cls_explainer.pred_probs.max().item()\n",
        "\n",
        "    # Separating positive and negative influence words\n",
        "    positive_words = [(word, score) for word, score in merged_attributions if score > 0]\n",
        "    negative_words = [(word, score) for word, score in merged_attributions if score < 0]\n",
        "\n",
        "    # Selecting the most influential words\n",
        "    top_positive = positive_words[:3]  # Top 3 words supporting the prediction\n",
        "    top_negative = negative_words[:2]  # Top 2 words that pushed against the prediction\n",
        "\n",
        "    # Construct explanation based on classification\n",
        "    explanation = f\"\" **This statement was classified as '{predicted_label}' with {confidence:.2%} confidence.**\\n\\n\"\n",
        "\n",
        "    if top_positive:\n",
        "        explanation += \"The model identified the following words as important in making this decision:\\n\"\n",
        "        for word, score in top_positive:\n",
        "            explanation += f\"\\\"{word}\\\" reinforced this classification.\"\n",
        "\n",
        "    if top_negative:\n",
        "        explanation += \"\\n**However, the following words slightly pushed against this classification:**\\n\"\n",
        "        for word, score in top_negative:\n",
        "            explanation += f\"- **\\\"{word}\\\"** was present but did not strongly contribute.\\n\"\n",
        "\n",
        "    explanation += \"This explanation is based on which words the model found important in making its prediction.\"\n",
        "\n",
        "    if visualize:\n",
        "        cls_explainer.visualize()\n",
        "\n",
        "    return explanation\n",
        "\n",
        "# Example usage:\n",
        "explanation = generate_explanation(\"Diabetes misinformation can be harmful, spreading false claims about causes, treatments, and cures.\")\n",
        "print(explanation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncjpxXJNmU2V"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import torch\n",
        "import os\n",
        "from transformers_interpret import SequenceClassificationExplainer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "#Load Model and Tokenizer\n",
        "model_name = \"/content/drive/MyDrive/new_model/\"  # Replace with actual model path\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()\n",
        "\n",
        "# Initialize Transformers-Interpret Explainer\n",
        "cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n",
        "\n",
        "#Function to merge subword attributions properly\n",
        "def merge_subword_attributions(attributions):\n",
        "    merged_tokens = []  #Initialize properly\n",
        "    merged_values = []  #Initialize properly\n",
        "    prev_word = None\n",
        "    accumulated_score = 0.0\n",
        "\n",
        "    for token, score in attributions:\n",
        "        clean_token = token.replace(\"##\", \"\")  # Remove subword markers\n",
        "\n",
        "        if token.startswith(\"##\") and prev_word:\n",
        "            prev_word += clean_token\n",
        "            accumulated_score += score\n",
        "        else:\n",
        "            if prev_word:\n",
        "                merged_tokens.append(prev_word)\n",
        "                merged_values.append(accumulated_score)\n",
        "            prev_word = clean_token\n",
        "            accumulated_score = score\n",
        "\n",
        "    if prev_word:\n",
        "        merged_tokens.append(prev_word)\n",
        "        merged_values.append(accumulated_score)\n",
        "\n",
        "    return list(zip(merged_tokens, merged_values))\n",
        "\n",
        "#Function to extract key words based on feature attribution scores\n",
        "def extract_key_words(text, top_n=5):\n",
        "    \"\"\"Extracts top influential words from the model's attributions.\"\"\"\n",
        "    attributions = cls_explainer(text)\n",
        "    merged_attributions = merge_subword_attributions(attributions)\n",
        "\n",
        "    #Now sorting works correctly\n",
        "    merged_attributions = sorted(merged_attributions, key=lambda x: abs(x[1]), reverse=True)\n",
        "    key_words = [word for word, score in merged_attributions[:top_n]]\n",
        "\n",
        "    # Compute total attribution score\n",
        "    total_attribution_score = sum(abs(score) for _, score in merged_attributions)\n",
        "\n",
        "    return key_words, total_attribution_score\n",
        "\n",
        "#Function to classify text using the fine-tuned BERT model\n",
        "def classify_with_bert(text):\n",
        "    \"\"\"Classifies text using the BERT model.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "    predicted_label_index = probabilities.argmax()\n",
        "    predicted_label = \"Misinformation\" if predicted_label_index == 1 else \"Real\"\n",
        "    confidence = probabilities[predicted_label_index]\n",
        "\n",
        "    return predicted_label, confidence\n",
        "\n",
        "#Function to test multiple variations of an input\n",
        "def test_multiple_cases(original_text, variations):\n",
        "    \"\"\"Runs the model on multiple variations of the text and selects the best prediction.\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for variation in variations:\n",
        "        predicted_label, confidence = classify_with_bert(variation)\n",
        "        key_words, total_attribution_score = extract_key_words(variation)\n",
        "\n",
        "        results.append({\n",
        "            \"text\": variation,\n",
        "            \"predicted_label\": predicted_label,\n",
        "            \"confidence\": confidence,\n",
        "            \"key_words\": key_words,\n",
        "            \"total_attribution_score\": total_attribution_score\n",
        "        })\n",
        "\n",
        "    #Sort by highest confidence + highest total attribution score\n",
        "    results = sorted(results, key=lambda x: (x[\"confidence\"], x[\"total_attribution_score\"]), reverse=True)\n",
        "\n",
        "    #Select the most confident prediction\n",
        "    best_result = results[0]\n",
        "\n",
        "    return best_result\n",
        "#Make sure to replace this with your actual API key\n",
        "OPENAI_API_KEY = \"\"\n",
        "\n",
        "#Explicitly pass the API key when initializing OpenAI client\n",
        "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "def generate_gpt_explanation(prediction, key_words, confidence):\n",
        "    \"\"\"Generates a user-friendly explanation using OpenAI's GPT model.\"\"\"\n",
        "\n",
        "    if not key_words:\n",
        "        return f\"The model classified this text as '{prediction}' but did not find strong influencing words for further explanation.\"\n",
        "\n",
        "    explanation_type = \"why this text might contain misinformation\" if prediction == \"Misinformation\" else \"why this text was classified as real\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    The model classified this text as '{prediction}' with {confidence:.2%} confidence.\n",
        "\n",
        "    The most important words influencing this decision were: {', '.join(key_words)}.\n",
        "\n",
        "    Provide a **concise, context-aware explanation** (50-80 words) about why these words suggest {prediction}.\n",
        "    - Explain **how these words are used in {prediction.lower()} content**.\n",
        "    - Provide **one real-world example** of {prediction.lower()} content using these words.\n",
        "    - Compare how these words are used in **real scientific news**.\n",
        "    - Make the explanation **brief and easy to understand**.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,  # Keeps responses consistent\n",
        "        max_tokens=100  # Limits response length (approx. 50-80 words)\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "\n",
        "def explain_text(original_text, variations, top_n=5):\n",
        "    \"\"\"Runs multiple test cases on the model before selecting the best one for explanation.\"\"\"\n",
        "\n",
        "    # Step 1: Test multiple cases and pick the best\n",
        "    best_case = test_multiple_cases(original_text, variations)\n",
        "\n",
        "    # Step 2: Extract final key words & confidence\n",
        "    key_words = best_case[\"key_words\"]\n",
        "    prediction = best_case[\"predicted_label\"]\n",
        "    confidence = best_case[\"confidence\"]\n",
        "\n",
        "    # Step 3: Generate Explanation Using GPT-3.5\n",
        "    gpt_explanation = generate_gpt_explanation(prediction, key_words, confidence)\n",
        "\n",
        "    return {\n",
        "        \"prediction\": prediction,\n",
        "        \"confidence\": f\"{confidence:.2%}\",\n",
        "        \"best_text\": best_case[\"text\"],\n",
        "        \"key_words\": key_words,\n",
        "        \"explanation\": gpt_explanation\n",
        "    }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLsYi7jMpvh6",
        "outputId": "99101a13-f4d7-45fc-c50d-5df5794c9401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'prediction': 'Misinformation', 'confidence': '99.87%', 'best_text': 'Diabetes misinformation can be harmful, spreading false claims about causes, treatments, and cures.', 'key_words': ['misinformation', 'diabetes', 'harmful', 'cures', 'claims'], 'explanation': 'In misinformation content, words like \"misinformation,\" \"harmful,\" \"cures,\" and \"claims\" are often used to promote false information about health topics like diabetes. For example, a misleading article might claim that a certain product can cure diabetes without scientific evidence. In contrast, real scientific news would use these words in a factual and evidence-based manner, discussing the potential harms of misinformation and the importance of evidence-based treatments for diabetes.'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "user_text = \"Diabetes misinformation can be harmful, spreading false claims about causes, treatments, and cures.\"\n",
        "\n",
        "explanation_result = explain_text(user_text, [user_text])\n",
        "\n",
        "print(explanation_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8jX3ijARcW2",
        "outputId": "93284704-9340-4982-82c5-1c79aeeed08a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement numpy.rec (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy.rec\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install numpy.rec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "RHWr87436irp",
        "outputId": "0853f31c-a5eb-41ca-bab2-221facf1c834"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\nNo module named 'numpy.rec'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidate_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAssistantVocabTranslatorCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/candidate_generator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_sklearn_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[0m\u001b[1;32m      6\u001b[0m                        \u001b[0mget_sum_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_sputils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_ulong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_namespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxp_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.rec'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdynamic_module_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustom_object_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompileConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftAdapterMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepspeed_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_deepspeed_zero3_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\nNo module named 'numpy.rec'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e4f15246829a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers_interpret\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequenceClassificationExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers_interpret/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mattributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLIGAttributions\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseExplainer\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m from .explainers.text.multilabel_classification import (  # noqa: F401\n\u001b[1;32m      4\u001b[0m     \u001b[0mMultiLabelClassificationExplainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers_interpret/explainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreTrainedTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\nNo module named 'numpy.rec'"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "import torch\n",
        "import os\n",
        "from transformers_interpret import SequenceClassificationExplainer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load Model and Tokenizer\n",
        "model_name = \"/content/drive/MyDrive/new_model/\"  # Replace with actual model path\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()\n",
        "\n",
        "# Initialize Transformers-Interpret Explainer\n",
        "cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n",
        "\n",
        "# Function to merge subword attributions properly\n",
        "def merge_subword_attributions(attributions):\n",
        "    merged_tokens = []\n",
        "    merged_values = []\n",
        "    prev_word = None\n",
        "    accumulated_score = 0.0\n",
        "\n",
        "    for token, score in attributions:\n",
        "        clean_token = token.replace(\"##\", \"\")\n",
        "\n",
        "        if token.startswith(\"##\") and prev_word:\n",
        "            prev_word += clean_token\n",
        "            accumulated_score += score\n",
        "        else:\n",
        "            if prev_word:\n",
        "                merged_tokens.append(prev_word)\n",
        "                merged_values.append(accumulated_score)\n",
        "            prev_word = clean_token\n",
        "            accumulated_score = score\n",
        "\n",
        "    if prev_word:\n",
        "        merged_tokens.append(prev_word)\n",
        "        merged_values.append(accumulated_score)\n",
        "\n",
        "    return list(zip(merged_tokens, merged_values))\n",
        "\n",
        "\n",
        "def extract_key_words(text, top_n=5):\n",
        "    \"\"\"Extracts top influential words from the model's attributions.\"\"\"\n",
        "    attributions = cls_explainer(text)\n",
        "    merged_attributions = merge_subword_attributions(attributions)\n",
        "\n",
        "    merged_attributions = sorted(merged_attributions, key=lambda x: abs(x[1]), reverse=True)\n",
        "    key_words = [word for word, score in merged_attributions[:top_n]]\n",
        "\n",
        "    total_attribution_score = sum(abs(score) for _, score in merged_attributions)\n",
        "\n",
        "    return key_words, total_attribution_score\n",
        "\n",
        "def should_explain(input_text, total_attribution_score, alpha=0.15):\n",
        "    \"\"\"Determines whether an explanation should be generated based on attribution strength.\"\"\"\n",
        "    num_words = len(input_text.split())\n",
        "    threshold = alpha * num_words  # Dynamic threshold based on input length\n",
        "    return total_attribution_score > threshold\n",
        "\n",
        "def classify_with_bert(text):\n",
        "    \"\"\"Classifies text using the BERT model.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "    predicted_label_index = probabilities.argmax()\n",
        "    predicted_label = \"Misinformation\" if predicted_label_index == 1 else \"Real\"\n",
        "    confidence = probabilities[predicted_label_index]\n",
        "\n",
        "    return predicted_label, confidence\n",
        "\n",
        "def test_multiple_cases(original_text, variations):\n",
        "    \"\"\"Runs the model on multiple variations of the text and selects the best prediction.\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for variation in variations:\n",
        "        predicted_label, confidence = classify_with_bert(variation)\n",
        "        key_words, total_attribution_score = extract_key_words(variation)\n",
        "\n",
        "        results.append({\n",
        "            \"text\": variation,\n",
        "            \"predicted_label\": predicted_label,\n",
        "            \"confidence\": confidence,\n",
        "            \"key_words\": key_words,\n",
        "            \"total_attribution_score\": total_attribution_score\n",
        "        })\n",
        "    results = sorted(results, key=lambda x: (x[\"confidence\"], x[\"total_attribution_score\"]), reverse=True)\n",
        "\n",
        "    best_result = results[0]\n",
        "\n",
        "    return best_result\n",
        "\n",
        "OPENAI_API_KEY = \"\"  # Replace with actual API key\n",
        "\n",
        "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "\n",
        "def generate_gpt_explanation(prediction, key_words, confidence, user_text):\n",
        "    \"\"\"Generates a user-friendly explanation using OpenAI's GPT model.\"\"\"\n",
        "\n",
        "    if not key_words:\n",
        "        return f\"The model classified this text as '{prediction}' but did not find strong influencing words for further explanation.\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    The model classified this text as '{prediction}' with {confidence:.2%} confidence.\n",
        "\n",
        "    **Original sentence:** \"{user_text}\"\n",
        "\n",
        "    The most important words influencing this decision were: {', '.join(key_words)}.\n",
        "\n",
        "    Provide a **concise, context-aware explanation** (50-80 words) about why these words suggest {prediction}.\n",
        "    - Explain how these words are used in {prediction.lower()} content.\n",
        "    - Provide one real-world example of {prediction.lower()} content using these words.\n",
        "    - Compare how these words are used in real scientific news.\n",
        "    - Make the explanation brief and easy to understand.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "        max_tokens=50\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def explain_text(original_text, variations, top_n=5):\n",
        "    \"\"\"Runs multiple test cases on the model before selecting the best one for explanation.\"\"\"\n",
        "\n",
        "    best_case = test_multiple_cases(original_text, variations)\n",
        "\n",
        "    key_words = best_case[\"key_words\"]\n",
        "    prediction = best_case[\"predicted_label\"]\n",
        "    confidence = best_case[\"confidence\"]\n",
        "    total_attribution_score = best_case[\"total_attribution_score\"]\n",
        "\n",
        "    if not should_explain(original_text, total_attribution_score):\n",
        "        return {\n",
        "            \"prediction\": prediction,\n",
        "            \"confidence\": f\"{confidence:.2%}\",\n",
        "            \"best_text\": best_case[\"text\"],\n",
        "            \"key_words\": key_words,\n",
        "            \"explanation\": \"The model's attribution scores were too low for a reliable explanation.\"\n",
        "        }\n",
        "\n",
        "    gpt_explanation = generate_gpt_explanation(prediction, key_words, confidence, original_text)\n",
        "\n",
        "    return {\n",
        "        \"prediction\": prediction,\n",
        "        \"confidence\": f\"{confidence:.2%}\",\n",
        "        \"best_text\": best_case[\"text\"],\n",
        "        \"key_words\": key_words,\n",
        "        \"explanation\": gpt_explanation\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "_HLKyCIA6kqo",
        "outputId": "fdba7945-fb0c-4713-c656-7e976ee16ce0"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'explain_text' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-59837471fcd3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mexplanation_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplain_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_variations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplanation_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'explain_text' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# Example Usage in Jupyter Notebook\n",
        "user_text = \"Diabetes misinformation can be harmful, spreading false claims about causes, treatments, and cures.\"\n",
        "test_variations = [\n",
        "    \"Some say diabetes can be cured with natural remedies, but is it true?\",\n",
        "    \"Doctors warn against unverified diabetes treatments claiming to be effective.\",\n",
        "    \"A new study suggests potential diabetes treatments, but more research is needed.\",\n",
        "    \"False claims about diabetes cures are spreading rapidly on social media.\"\n",
        "]\n",
        "\n",
        "explanation_result = explain_text(user_text, test_variations)\n",
        "print(explanation_result)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
